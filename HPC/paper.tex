\documentclass[sigconf]{acmart} %\documentclass[sigconf, language=german]{acmart}
\usepackage{booktabs}

\begin{document}

\title{MPI+OpenMP in MPI4.0}
\author{Dreßler Justus}
\orcid{0009-0005-0657-5854}
\affiliation{
    \institution{Friedrich Schiller University}
    \department{Faculty of Mathematics and Computer Science}
    \streetaddress{Fürstengraben 1, Main University Building (Universitätshauptgebäude)}
    \postcode{07743}
    \city{Jena}
    \country{Germany}
}
\email{justus.dressler@uni-jena.de}

\keywords{MPI, MPI+OpenMP,MPI+Threads}

\begin{CCSXML}
    <ccs2012>
    <concept>
    <concept_id>10010147.10010169</concept_id>
    <concept_desc>Computing methodologies~Parallel computing methodologies</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
    </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Parallel computing methodologies}

\begin{abstract}

    %TODO: add abstract
    Lorem Ipsum dolor sit amet, consectetur adipiscing elit.
    Mauris dictum quam eget orci porttitor ultrices.
    Suspendisse purus nisl, dapibus et rhoncus in, eleifend id quam. Integer volutpat dolor urna, non auctor magna sodales non.
    Ut in fermentum erat, lacinia malesuada urna.
    Nullam et dolor ut purus fermentum rutrum.
    Suspendisse non velit sollicitudin, rutrum est at, commodo sem. Pellentesque viverra tincidunt mauris ut egestas.
    Integer vel orci sapien. Sed ut mi at lacus vehicula fermentum.
    Integer euismod vestibulum auctor.
    Nunc vulputate leo eget aliquam bibendum.

\end{abstract}

\maketitle

% Introduction, roughly 1 page
\section{Introduction}

Supercomputing relies on multiple layers of parallelisation, one intranode and one internode.
Historically this was handled fine with a MPI+MPI (or MPI-Everywhere) programming model.
In modern supercomputing rising core counts neccessitates a more nuanced programming model, one that makes scaling across cores on a node easier.
This caused MPI+OpenMP to become the new de facto standard programming model in Supercomputing.
It simplifies the seperation of internode and intranode tasks greatly, but it also imposes new challenges for developers.
This paper tries to summarize the current (MPI4.0) methods to overcome these challenges and unlock high performance MPI+OpenMP applications.
% this feels like making a buzzword list


\section{Motivation}
% write why parallel communications is such a large burdon

% Body, roughly 2.5 pages
\section{Parallel Communication in MPI4.0}

% focus on CPU parallelisation
\subsection{Communicator}
% see Presentation
\subsection{Tags}
% see Presentation
\subsection{Partitioned Operations}
% see Presentation
\subsection{Sessions} %no MPI Endpoints, since they didn't make the cut for MPI4.0 and are thus not relevant to current parallelisation

\section{Working with accelerators}
% GPU parallelisation (does this fill an extra chapter though, or would it rather be buried in PartitionedOperations)
% One might write something to MPIX_Stream?

% I'd love to write about actual performance, but I doubt that I can get a working example on Ara in under a week
% Also I straight up wouldn't know how to benchmark it properly

% End, roughly 0.5 pages
\section{Conclusions}
\section{References}

\end{document}
