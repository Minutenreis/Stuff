@manual{mpi40,
  organization = {Message Passing Interface Forum},
  title        = {{MPI}: A Message-Passing Interface Standard Version 4.0},
  url          = {https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf},
  year         = 2021,
  month        = jun
}

@article{zambreLogicalParallel2021,
  author   = {Zambre, Rohit and Sahasrabudhe, Damodar and Zhou, Hui and Berzins, Martin and Chandramowlishwaran, Aparna and Balaji, Pavan},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  title    = {Logically Parallel Communication for Fast MPI+Threads Applications},
  year     = {2021},
  volume   = {32},
  number   = {12},
  pages    = {3038-3052},
  keywords = {Parallel processing;Supercomputing;Programming;Semantics;Upper bound;MPI+threads;MPI+OpenMP;exascale MPI;MPI_THREAD_MULTIPLE;MPI endpoints;Uintah;HYPRE;wombat;Legion},
  doi      = {10.1109/TPDS.2021.3075157}
}

@inproceedings{Zhou2022,
  series     = {EuroMPI/USA'22},
  title      = {MPIX Stream: An Explicit Solution to Hybrid MPI+X Programming},
  url        = {http://dx.doi.org/10.1145/3555819.3555820},
  doi        = {10.1145/3555819.3555820},
  booktitle  = {Proceedings of the 29th European MPI Users' Group Meeting},
  publisher  = {ACM},
  author     = {Zhou, Hui and Raffenetti, Ken and Guo, Yanfei and Thakur, Rajeev},
  year       = {2022},
  month      = sep,
  collection = {EuroMPI/USA'22}
}

@inproceedings{MPISessions,
  author    = {Huber, Dominik and Streubel, Maximilian and Compr\'{e}s, Isa\'{\i}as and Schulz, Martin and Schreiber, Martin and Pritchard, Howard},
  title     = {Towards Dynamic Resource Management with MPI Sessions and PMIx},
  year      = {2022},
  isbn      = {9781450397995},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3555819.3555856},
  doi       = {10.1145/3555819.3555856},
  abstract  = {Job management software on peta- and exascale supercomputers continues to provide static resource allocations, from a program’s start until its end. Dynamic resource allocation and management is a research direction that has the potential to improve the efficiency of HPC systems and applications by dynamically adapting the resources of an application during its runtime. Resources can be adapted based on past, current or even future system conditions and matching optimization targets. However, the implementation of dynamic resource management is challenging as it requires support across many layers of the software stack, including the programming model. In this paper, we focus on the latter and present our approach to extend MPI Sessions to support dynamic resource allocations within MPI applications. While some forms of dynamicity already exist in MPI, it is currently limited by requiring global synchronization, being application or application-domain specific, or by suffering from limited support in current HPC system software stacks. We overcome these limitations with a simple, yet powerful abstraction: resources as process sets, and changes of resources as set operations leading to a graph-based perspective on resource changes. As the main contribution of this work, we provide an implementation of this approach based on MPI Sessions and PMIx. In addition, an illustration of its usage is provided, as well as a discussion about the required extensions of the PMIx standard. We report results based on a prototype implementation with Open MPI using a synthetic application, as well as a PDE solver benchmark on up to four nodes and a total of 112 cores. Overall, our results show the feasibility of our approach, which has only very moderate overheads. We see this first proof-of-concept as an important step towards resource adaptivity based on MPI Sessions.},
  booktitle = {Proceedings of the 29th European MPI Users' Group Meeting},
  pages     = {57–67},
  numpages  = {11},
  keywords  = {malleability, dynamic resources, PMIx, MPI Sessions},
  location  = {Chattanooga, TN, USA},
  series    = {EuroMPI/USA '22}
}

@misc{zambreLessonsLearned2022,
  title         = {Lessons Learned on MPI+Threads Communication},
  author        = {Rohit Zambre and Aparna Chandramowlishwaran},
  year          = {2022},
  eprint        = {2206.14285},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/abs/2206.14285}
}

@inproceedings{hypre2020,
  author    = {Sahasrabudhe, Damodar
               and Berzins, Martin},
  editor    = {Krzhizhanovskaya, Valeria V.
               and Z{\'a}vodszky, G{\'a}bor
               and Lees, Michael H.
               and Dongarra, Jack J.
               and Sloot, Peter M. A.
               and Brissos, S{\'e}rgio
               and Teixeira, Jo{\~a}o},
  title     = {Improving Performance of the Hypre Iterative Solver for Uintah Combustion Codes on Manycore Architectures Using MPI Endpoints and Kernel Consolidation},
  booktitle = {Computational Science -- ICCS 2020},
  year      = {2020},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {175--190},
  abstract  = {The solution of large-scale combustion problems with codes such as the Arches component of Uintah on next generation computer architectures requires the use of a many and multi-core threaded approach and/or GPUs to achieve performance. Such codes often use a low-Mach number approximation, that require the iterative solution of a large system of linear equations at every time step. While the discretization routines in such a code can be improved by the use of, say, OpenMP or Cuda Approaches, it is important that the linear solver be able to perform well too. For Uintah the Hypre iterative solver has proved to solve such systems in a scalable way. The use of Hypre with OpenMP leads to at least 2x slowdowns due to OpenMP overheads, however. This behavior is analyzed and a solution proposed by using the MPI Endpoints approach is implemented within Hypre, where each team of threads acts as a different MPI rank. This approach minimized OpenMP synchronization overhead, avoided slowdowns, performed as fast or (up to 1.5x) faster than Hypre's MPI only version, and allowed the rest of Uintah to be optimized using OpenMP. Profiling of the GPU version of Hypre showed the bottleneck to be the launch overhead of thousands of micro-kernels. The GPU performance was improved by fusing these micro kernels and was further optimized by using Cuda-aware MPI. The overall speedup of 1.26x to 1.44x was observed compared to the baseline GPU implementation.},
  isbn      = {978-3-030-50371-0}
}

@manual{intelOmniPath,
  title        = {Intel® Omni-Path Fabric Host Software User Guide},
  organization = {Intel Corporation},
  edition      = {16.0},
  year         = {2020},
  month        = {Jan},
  note         = {page 61},
  url          = {https://www.intel.com/content/dam/support/us/en/documents/network-and-i-o/fabric-products/Intel_OP_Fabric_Host_Software_UG_H76470_v16_0.pdf}
}

@misc{intelXeon6,
  title        = {Intel® Xeon® 6},
  organization = {Intel Corporation},
  url          = {https://ark.intel.com/content/www/us/en/ark/products/series/240357/intel-xeon-6.html#@Server},
  note         = {Accessed: 2024-07-12},
  year         = {2024}
}

@misc{amd4thGenEpyc,
  organization = {Advanced Micro Devices, Inc. (AMD)},
  url          = {https://www.amd.com/en/products/processors/server/epyc/4th-generation-9004-and-8004-series.html#specs},
  title        = {4th Generation AMD EPYC™ Processors},
  note         = {Accessed: 2024-07-12},
  year         = {2024}
}